---
title: "Spatial modeling of K-12 school shootings from 1990-2019 as a Matern clustered point process"
author: J Steven Raquel
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, results = 'asis',fig.height = 3, fig.width = 6)
library(tidyverse)
library(lubridate)
# spatial data libraries
library(sf)
library(sp)
library(spdep)
library(raster) 
library(maps) # to get map shapefiles
library(maptools)
library(spatstat) # spatial statistics
library(patchwork) # for plotting ggplots
library(RColorBrewer)
library(classInt)
library(bookdown)
library(knitr)
library(kableExtra)
```

```{r read_incidents_old, eval = F, include = F}
incidents <- read_csv("ss_incidents.csv", na = c("", "null", "N/A"),
                      show_col_types = F)

# incident dataset filtered
incidents_f <- incidents %>%
  dplyr::select(Incident_ID, Date, Quarter, School, Narrative,
                City, State, School_Level, Location, Location_Type, 
                During_School, Time_Period, Situation, 
                Bullied, Domestic_Violence, Gang_Related, 
                Preplanned, Shots_Fired) %>%
  mutate(Date = as.Date(Date)) %>%
  filter(!(Situation %in% c("Accidental", "Suicide/Attempted",
                            "Intentional Property Damage"))
         ) %>%
  filter(Gang_Related == "No") %>% 
  unite(Full_Location, c(School, City, State), remove = F, sep = " ") %>%
  mutate(Full_Location = as.character(Full_Location)) %>%
  filter(!(State %in% c("AK", "HI"))) %>%
  replace_na(list(Situation = "Unknown"))

incidents_f <- incidents_f %>% 
  mutate_at(setdiff(names(incidents_f), c("Incident_ID", "Full_Location", "Date")), 
               .funs = as.factor)
```

```{r get-lat-lon, eval = F, include = F}
# using Google Maps API to search up the lat/lon of all the schools
#incidents_f2 <- incidents_f %>% 
#  mutate_geocode(Full_Location)
# write.csv(incidents_f2, "ss_incidents2.csv", row.names = F)
```

```{r get-perps-weapons, eval = F}
perps <- read_csv("ss_perps.csv", na = c("", "null", "N/A"), 
                  show_col_types = F)
weapons <- read_csv("ss_weapons.csv", na = c("", "null", "N/A"),
                    show_col_types = F)

# perpetrator dataset filtered
perps_f <- perps %>% 
  dplyr::select(incidentid, age, gender, race, schoolaffiliation) %>%
  rename(Incident_ID = incidentid, Age = age, Gender = gender, 
         Race = race, School_Affiliation = schoolaffiliation)

# weapon dataset filtered
weapons_f <- weapons %>%
  dplyr::select(incidentid, weapontype) %>%
  rename(Incident_ID = incidentid, Weapon_Type = weapontype)

# adding perpetrator information to the dataset
incidents_f3 <- left_join(incidents_f2, perps_f, by = "Incident_ID")
# adding weapon information to the dataset
incidents_f4 <- left_join(incidents_f3, weapons_f, by = "Incident_ID")

```

```{r reading_data, warnings = F}
df <- read_csv("ss_incidents2.csv",
                  col_types = "ciDffffffffffffffffdd")

df_9019 <- df %>% filter(Date >= "1990-01-1" & Date <= "2019-12-31")
df_90S <- df %>% filter(Date >= "1990-01-01" & Date <= "1999-12-31")
df_00S <- df %>% filter(Date >= "2000-01-01" & Date <= "2009-12-31")
df_10S <- df %>% filter(Date >= "2010-01-01" & Date <= "2019-12-31")

gun_control <- read_csv("gun_control_database.csv", col_types = "cf")
```

```{r sf-wrangling}
# sf is loaded
# CRS for albers is 5070
# default for us states is 4269
data(us_states)

sf_9019 <- st_as_sf(df_9019, coords = c("lon", "lat")) %>% 
  st_set_crs(4269)

sf_90S <- st_as_sf(df_90S, coords = c("lon", "lat")) %>%
  st_set_crs(4269)

sf_00S <- st_as_sf(df_00S, coords = c("lon", "lat")) %>%
  st_set_crs(4269)

sf_10S <- st_as_sf(df_10S, coords = c("lon", "lat")) %>% 
  st_set_crs(4269)

```

# Introduction

The prevalence of gun violence in schools in the United States has been referred to both as an epidemic and a public health crisis, and one that has steadily increased over the past several decades. Apart from the trauma that such an event can bring to a community, there is also resonant fear that such incidents inspire copycat events on a local and a larger scale. This spatial analysis attempts to model the incidence of these shootings as a Poisson point process, in order to ascertain whether the locations and events occur with complete spatial randomness, and thereafter create a model with which these events can be predicted. Ultimately, a Cox Matern cluster process model was decided upon, which lead us to conclude that in fact, school shooting events do give rise to future school shootings around them. This spatial analysis seeks to model on a spatial level these incidences, so that we can better understand where these tragedies are likely to happen such that we can avert them and make the country a safer place for young people.

# Methods

## Data

The school shooting data was sourced directly from the "K-12 School Shooting Database" made available by the Center for Homeland Defense and Security (CHDS), and was specifically subset to between the years of 1990-2019. The information that comprises the dataset was determined by a specific process which entailed asking what exactly comprises a school shooting. Although the original database contains shootings ruled accidental (from misuse of a firearm) as well as incidences of gang-related gun violence on school grounds, among other incidents, we did not opt to consider this data as relevant to this study in particular. Targeted events related to domestic situations, or the escalation of disputes (e.g. fistfights in which one person pulls out a firearm) were also ruled school shootings for the purposes of this study. 

## Exploratory Data Analysis

```{r plot-point-pattern-creation, echo = F}
# looking at 3 decades of school shootings
# 1990-1999
ss_plot_90s <- ggplot() + 
  geom_sf(data = us_states) +
  geom_sf(data = sf_90S, size = 0.75, col = "red") + 
#  geom_sf(data = cities_sf, color = "black", size = 1) +
#  geom_text_repel(data = cities_sf, 
#                  aes(x = x, y = y, label = City_State),
#                  hjust = 1, vjust = -0.25, size = 2.5) +
  theme(axis.title = element_blank()) +
  ggtitle("1990-99, Total: 147") +
  theme_bw() +
  theme(
    axis.title = element_blank(),
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank(), 
    axis.text.y = element_blank(),  
    axis.ticks.y = element_blank(),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank())

# 2000-2009
ss_plot_00s <- ggplot() + 
  geom_sf(data = us_states) +
  geom_sf(data = sf_00S, size = 0.75, col = "red") + 
#  geom_sf(data = cities_sf, color = "black", size = 1) +
#  geom_text_repel(data = cities_sf, 
#                  aes(x = x, y = y, label = City_State),
#                  hjust = 1, vjust = -0.25, size = 2.5) +
  theme(axis.title = element_blank()) +
  ggtitle("2000-09, Total: 186") +
  theme_bw() +
  theme(
    axis.title = element_blank(),
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank(), 
    axis.text.y = element_blank(),  
    axis.ticks.y = element_blank(),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank())

# 2010-2019
ss_plot_10s <- ggplot() + 
  geom_sf(data = us_states) +
  geom_sf(data = sf_10S, size = 0.75, col = "red") + 
#  geom_sf(data = cities_sf, color = "black", size = 1) +
#  geom_text_repel(data = cities_sf, 
#                  aes(x = x, y = y, label = City_State),
#                  hjust = 1, vjust = -0.25, size = 2.5) +
  ggtitle("2010-19, Total: 240") + 
  theme_bw() +
  theme(
    axis.title = element_blank(),
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank(), 
    axis.text.y = element_blank(),  
    axis.ticks.y = element_blank(),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank())

# 1990-2019
ss_plot_9019 <- ggplot() + 
  geom_sf(data = us_states) +
  geom_sf(data = sf_9019, size = 0.75, col = "red") + 
#  geom_sf(data = cities_sf, color = "black", size = 1) +
#  geom_text_repel(data = cities_sf, 
#                  aes(x = x, y = y, label = City_State),
#                  hjust = 1, vjust = -0.25, size = 2.5) +
  ggtitle("1990-2019, Total: 573") + 
  theme_bw() +
  theme(
      axis.title = element_blank(),
      axis.text.x=element_blank(), 
      axis.ticks.x=element_blank(), 
      axis.text.y=element_blank(),  
      axis.ticks.y=element_blank(),
      panel.border =  element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank())
```

```{r plots-all-four}
# patchwork is loaded
ss_plot_90s + ss_plot_00s + ss_plot_10s + ss_plot_9019 +
  plot_annotation(
    title = "K-12 school shootings in the US",
    caption = "Source: The Department of Homeland Security."
  )
```

As we can see from the plot above, the total number of shootings per decade has not only steadily increased over the past 30 years, but the events also occur in and around the same places, which gives credence to our hypothesis that the events exhibit a clustered pattern. We notice in fact, that there are areas that seem relatively untouched by school shootings in the western United States, whereas shootings all across the South, Midwest and the East Coast recur a great deal. While the West Coast is somewhat blighted by school shootings, particularly in the San Francisco and Los Angeles metropolitan areas, along with major cities in the Pacific Northwest), it is not nearly at the rate experienced by the other side of the US. 

One thing that it is important to note, as with many spatial analyses that relate to events caused by humans, that the rate of these events do have a high correlation with population density, i.e. there are more observations of school shooting events in areas where many people live. While the implications of this unfortunately will not be well explored in this literature, it is important to take note of as a confounding factor when asking questions about the frequency of these events. 

## Point Pattern Analysis

With respect to the coordinate-level data, as with any spatial point pattern analysis, we are concerned with the following three questions, 1) whether the points are located at random, 2) whether they are clustered, and 3) whether they are placed regularly. The hypothesis of _complete spatial randomness_, or a homogeneous Poisson process, asserts the following:

* The number of events in any region $S$ with area $|S|$ follows a Poisson distribution with mean $\lambda |S|$, where $\lambda$ is the intensity, i.e. $\lambda$ does not change over $S$
* Given $n$ events in $S$, the points $s_i$ are independently located according to the uniform distribution on $S$, i.e. there is no interaction amongst events.

The intensity function $\lambda(s)$, also known as the first-order property of the spatial point process, is defined as

$$\lambda(s) = \lim_{|\Delta s| \to 0} \frac{E[N(\Delta s)]}{| \Delta s|}$$

Firstly we want to ascertain whether the incidences of school shootings are indeed a Poisson process, and if so, determine whether or not the process is homogeneous or inhomogeneous. 

```{r create-ppp, warning = F}
# reading the data
# from readr (part of tidyverse)
df_9019 <- df %>% filter(Date >= "1990-01-1" & Date <= "2019-12-31")

## getting shapefile of the US
us_map <- map("usa",plot=F)

## These are the vectors with the longitudes and latitudes
us_boundaries_x <- us_map$x
us_boundaries_y <- us_map$y

## Creating the matrix with the longitude and latitude of the boundaries
us_poly <- matrix(cbind(us_boundaries_x[1:6886],us_boundaries_y[1:6886]),
                        nrow=6886,ncol=2)
## Creating the window of the spatial point pattern.
us_win <- owin(poly=us_poly)

# creating ppp objects with the coordinates and the windows
ppp_9019 <- ppp(x = df_9019$lon, y = df_9019$lat,
                window = us_win,
                marks = df_9019 %>% dplyr::select(Preplanned))
```

```{r plot-simulate-ppm}
# homogenous poisson process
fit_hpp <- ppm(ppp_9019 ~ 1)
sim1 <- simulate(fit_hpp)
plot(sim1, use.marks = F,
     main = "Simulation of an inhomogeneous Poisson process")
```
This plot demonstrates but an example of what a homogeneous Poisson process, which has complete spatial randomness, would look like. We note that the distribution of events is scattered all throughout the space such that there are no clusters anywhere, and moreover we do not see any specific pattern in their distribution. We want to test formally that the data does not follow such a distribution. This is in pursuit of our ultimate goal of creating a model that can accurately demonstrate the distribution of these events.

### Kernel Density

One method by which we can visually ascertain as to whether the point pattern $X$ is homogeneous is by looking at the plot of the Gaussian kernel smoothed intensity function, which appears as a heatmap. Density based mesasures look at the first order property of the process, which illustrate how observations vary from place to place due to the underlying property, whereas the second order property illustrates how observations vary from place to place due to interaction effects between observations themselves (Gimond).

The smoothing bandwidth $\sigma$, the standard deviation of the isotropic Gaussian kernel, is chosen to minimise the mean square error (MSE) as defined by Diggle (1985), which is calculated as follows

$$MSE(\sigma) = MSE(\sigma)/\lambda^2 - g(0)$$

where $\lambda$ is the mean intensity and $g$ is the pair correlation function.

```{r bandwidth-calculate, cache = T}
# optimal bandwidth for the kernel density of each point pattern
bw_opt_9019 <- bw.diggle(ppp_9019)
```


```{r density-90-19}
par(mfrow = c(1, 1)) # 2 rows 1 column
# heatmap of events in general
dens_9019 <- density.ppp(ppp_9019, bw = bw_opt_9019)
plot(dens_9019, main = "Heatmap of School Shootings, 1990-2019")
contour(dens_9019, add = T)
```

Based on this heatmap of the density function, we note that clustering is most strong around the south, namely Florida, as well as in the northeastern United States around New York. The same is also true for the Pacific Northwest. 

### Quadrats

While we could estimate the intensity function across the entire area, what we are interested in this particular analysis is to how the intensity varies across different regions contained therein. We do this by splitting up the area into what are referred to as _quadrats_, small subsets of the event space, and counting the number of events contained within each quadrat. We can test against the hypothesis of complete spatial randomness by generating a test statistic based on the number of expected vs observed events in each quadrat. The quadrat plot is given by the following plot:

```{r plot-quadrats-1990-2019}
quadrat_9019 <- quadratcount(ppp_9019)
plot(ppp_9019, axes = F, 
     main = "Quadrat Plot of Incidents, 1990-2019", 
     cols= rgb(0,0,0,.2), pch = 20,
     use.marks = F)
plot(quadrat_9019, add = TRUE)
```

It is important to note however that within a given quadrat, the number of events contained therein does not give us information about how clustered the events are inside that quadrat itself. Furthermoe,the count of events contained within each quadrat is also dependent on the definition of these dimensions, so they can be subject to misleading conclusions as a result. Since the continental United States is not shaped like a simple polygon, dividing it into a reasonable number of equitable and reasonably defined quadrats is not an easy task and this is a shortcoming we have to recognize. 

One methodology for testing for clustering is a Monte Carlo quadrat test in which we take a number of simulations of patterns under the null hypothesis e.g. the homogeneous Poisson point pattern we observed, and compute a $\chi^2$ test statistic based on the expected and observed counts in each quadrat, and their residuals. The alternative hypothesis varies depending on the test, but we want to determine specifically whether 1) the pattern is homogeneous or not and 2) whether the pattern is clustered. 

```{r tbl-quadrat-test, resuts = 'asis'}
# Ha: the pattern is not completely random
q_test_CSR <- quadrat.test(ppp_9019, alternative = "two.sided",
  method = "MonteCarlo", nsim = 4999)

# Ha: the pattern is 
# Ha: the PP is clustered
q_test_clustered <- quadrat.test(ppp_9019, alternative = "clustered",
             method = "MonteCarlo", nsim = 4999)

# hypotheses
hyp_null <- c("$X$ is a homogeneous Poisson process")
hyp_alt1 <- c("$X$ is not a homogeneous Poisson process")
hyp_alt2 <- c("$X$ is a clustered point pattern")

# data.frame of what will be in the table
q_test_df <- data.frame(H0 = rep(hyp_null, 2), 
                        HA = c(hyp_alt1, hyp_alt2),
                        Test_Statistic = c(q_test_CSR$statistic, 
                                           q_test_clustered$statistic),
                        p_value = c(q_test_CSR$p.value, 
                                    q_test_clustered$p.value),
                        Conclusion = c("reject $H_0$", "reject $H_0$"))

# plotting kable
q_test_df %>% 
  kbl(
    format = "latex",
    booktabs = T,
      col.names = c("Null hypothesis", "Alternative hypothesis", 
                    "Test Statistic", "p-Value", "Conclusion")) %>%
  kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
```

Based on this table, we have the results of two separate quadrat tests for our point pattern $X$, with the null hypothesis of complete spatial randomness against the corresponding alternative hypotheses of 1) $X$ not being a homogeneous Poisson process and 2) $X$ being a clustered point pattern.

In both cases, our test returns a corresponding p-value of approximately zero, so we have evidence to reject $H_0$ in both cases and conclude that we have respectively in $X$, an inhomogeneous Poisson process, and a clustered point pattern. 

### Ripley's K-function and the G-function

Ripley's K-function of a point process is defined so that $\lambda K(r)$ equals the expected number of additional random points within a distance $r$ of a typical random point of the point process $X$, and is determined by the second order moment properties of $X$. Deviations between the empirical and theoretical K function give us evidence of spatial clustering or regularity.

$$K(r) = \lambda r ^2, \lambda_2(r) = \lambda^2$$

There are various transformations of the K-function, for example the L-function proposed by Julian Besag, which stabilizes the variance, or the G-function, which is the cumulative distribution function of the distance from a typical random point of $X$ to the nearest other point of $X$. The G-function allows us to visually test for clustering on top of testing for deviation from complete spatial randomness (CSR). Moreover, the K and L-functions are rather computationally expensive relative to the G-function for this many events, so it will be our function of choice to demonstrate here. 

The G-function is given by

$$G(r) = 1 - \exp(-\lambda  \pi r^2) = 1 - \exp(-\pi K(r))$$

We can generate an "envelope" of simulated G-functions based on a random Poisson point pattern, and compare our empirical G-function from the data to ascertain whether the point pattern $X$ has complete spatial randomness, and to diagnose whether the point pattern is clustered. 

```{r G-envelope}
env_G <- envelope(ppp_9019, fun = Gest, verbose = F)
```

```{r plot-G-function}
par(mfrow = c(1,1))
plot_env_G <- plot(env_G, main = "envelope of G function",
                   xlim = c(0,2))
```

Contained in this figure we do see a marked difference between the respective G-functions of the empirical Poisson point process, and that of the theoretical point process which exhibits complete spatial randomness. Namely the empirical G-function exceeds the theoretical G-function up to approximately $r = 1$, after which point it crosses the envelope of the point process which exhibits CSR and emerges underneath it. A G-function predominantly underneath the envelope would be considered a regular point pattern, but we feel this plot moreso represents evidence towards it being clustered, which is corroborated by the results of the quadrat test earlier.

## Modeling

### The inhomogeneous Poisson point process

Given that we have confirmed via the quadrat tests and G-function that the Poisson process is inhomogeneous as well as clustered, our goal at this point is to develop a model with which we can estimate the intensity function $\lambda(s)$. 

There are two different methods that we can model this clustered process: the first. the inhomogeneous Poisson process, assumes that the process varies spatially as a function of certain covariates, and assumes that the events themselves are independent. The second method, the Cox process, which is itself a generalization of the inhomogeneous Poisson process, treats the intensity function $\lambda(s)$ itself as a stochastic process that we can model in the same manner as the first method. The latter also assumes that the events are not independent of each other. A more benign example of such a process might be the growth of a forest, since trees leave seeds around them which then can grow into even more trees.

For an inhomogeneous Poisson process, given the number of events $N(B)$ in a subset $B$ of the spatial domain $S$, the likelihood of an inhomogeneous point process is given by

$$P(N(B) = n) \Pi^n_{i=1} P(x_i = s_i) = \frac{1}{n!} \exp(- \int_B \lambda(s) ds) \Pi^n_{i=1} \lambda(s_i)$$

and the log-likelihood is proportional to

$$\log(\lambda(s))= \sum_{j=1}^p \beta_j x_j(s)$$

We can then model the intensity function as

$$\log (\lambda(s)) = \sum_{j=1}^p \beta_j x_j(s)$$

where $x_j(s), j = 1,...p$ are $p$ covariates, such that the log-likelihood is a function of the parameter coefficients $\beta_j$.

Here we fit a clustered inhomogeneous Poisson point process model, using the Matern cluster algorithm. We do not use any other covariates other than the coordinates in this model. We want to analyze the K-function based on this fitted model so that we can diagnose it and proceed. 

```{r fit2-kppm}
ppp_9019_um <- unmark(ppp_9019)
fit2 <- kppm(ppp_9019_um ~ x + y, "MatClust")
fit2_sum <- summary(fit2)
```

```{r fit2-diagnostic, cache = T}
fit2_env <- envelope(ppp_9019_um, fun = Kinhom,
                     funarges = list(lambda=fit2),
                     global = T, nsim = 10, verbose = F)
```

```{r plot-fit2-envelope}
plot(fit2_env)
```

This figure depicts an observed K-function as well as an envelope of theoretical K-functions based on the intensity function $\lambda(s)$ fit in the cluster model that was just fit. As can be seen here, the level of clustering is actually greater in the observed point process up until the distance $r=3$. We're not entirely satisfied by the results of this model due to how it underestimates the clustering relative to that expressed in the true data, hence we want to move on to the Matern cluster point process-- although the results of this model do have implications for the subsequent model. 

### Cox processes and the Matern cluster process model

The Cox process model treats the intensity function $\lambda(s)$ as a stochastic process, adding a significant layer of complexity (and flexibility) relative to the somewhat inflexible inhomogeneous Poisson process model. More specifically we will be discussing the Matern cluster process model. 

The Matern cluster point process is formed by taking a pattern of "parent" points, generated according to some Poisson process with intensity parameter $\kappa$, and then generating around it a random number of "offspring" which is itself a Poisson random variable with mean $\mu$. The locations of the offspring are independent and identically distributed via a Uniform distribution in a radius around the parent defined by the parameter $R$, also known as the scale.

Our goal is to minimize the discrepancy between the estimated model and the data, given some constraints. This discrepancy criterion $D(\theta)$ is given by

$$D(\theta) = \int_0^{r_0} w(t)[(\hat K(t))^c - (K(t;\theta))^c]^2 dt$$

where we have some parameters $r_0$, $c$, and the weight function $w(r)$. Minimizing this function is known as the method of minimum contrast. 

It works by first computing the K function, and then deriving the theoretical expected K value under the point process model. The model is then fit by tuning the optimal parameter values which minimizes the difference between the theoretical and empirical K-functions.

The theoretical K-function of this process is given by 

$$K(r) = \pi r^2 + \frac{h\large(\frac{r}{2R}\large)}{\kappa}$$

and the theoretical intensity of the process is $\lambda = \kappa \mu$.

Recall that earlier we fit an inhomogeneous Poisson point process model using the Matern cluster algorithm to define the clusters. The model that was fit returned estimates for not only the intensity function $\lambda(s)$, but also $\kappa$ and $\mu$, the intensity parameter of the parent points' pattern, and the mean parameter for the Poisson random variable of the number of offspring. For values of $c$ and $w(t)$, we want to opt for $c=0.25$ and a weight function of $w(t) = 1$, as these are well suited to well-clustered data.

```{r fit-matern-cluster}
fit2_kappa <- fit2$par['kappa'] %>% unname()
fit2_R <- fit2$par['R'] %>% unname()
# clustered Matern process model
fit_matclust <- matclust.estK(ppp_9019, 
                              lambda = fit2$lambda,
                              c(kappa=fit2_kappa, 
                                scale=fit2_R),
                              q = 1/4, p = 1)
```

```{r plot-matern-process}
# teal line is the homogenous PP
# red line is the 
plot(fit_matclust,
     main = "Empirical and theoretical K-functions \n Matern cluster process")
```
This figure gives the theoretical and empirical K-functions of the Matern cluster process model. The cyan line is the homogeneous Poisson process, which we have long since established is ill-fitted to the data. The black line is our empirical K-function, and the red line is the theoretical K-function under the model. 

Based off of this plot of the K-function, it appears in fact that the Matern cluster process model is fairly consistent at estimating the true underlying process given that the discrepancy between the theoretical and empirical K-functions is very low even up to very large distances. The following table gives the parameter values chosen for the Matern cluster model we have opted for. 

```{r tbl-matern-process}
matclust_df <- data.frame(kappa = fit2_kappa,
                         R = fit2_R,
                         c = 1/4,
                         p = 1)

matclust_df %>%
  kbl(booktabs = T,
      col.names = c("kappa", "R", 
                    "c", "w(t)")) %>%
  kable_styling(latex_options = c("striped"))
```

# Discussion 

It's been debated at length as to whether incidences of school shootings give rise to future events, as long as they've been happening in the modern day. With respect to how this model applies in context, we have established that it's indeed possible that this is the case, as demonstrated by how well the proposed Matern process model fits to the empirical process. Ascertaining the reason as to why this happens would have to be left for some future work, perhaps incorporating some of the unused categorical variables that were included in the data, such as whether the event was preplanned. The data can also be looked at on an areal level and compared next to the accessibility of both mental healthcare and guns (as shown in the Appendix). This was considered as an initial objective, but was later sidelined in favor of focusing on analysis of the clustering process.

One also wonders how one might model these tragedies from a spatiotemporal perspective, as it is well documented how events have been on the rise in the past several years (also shown in the Appendix). The ultimate goal of course, would be to be able to effectively isolate the pattern and frequency of these events to pre-empt their occurrences and minimize further harm to others, especially young people.

## Conclusion

This Matern cluster process model should be seen first and foremost as a building block upon which future work by individuals in public health, sociology, criminology, etc. can build. As with any instances of tragedy, many unresolved questions remain. Our hope is that some level of positive inspiration can happen from studying these events such that we can become better equipped at averting and dealing with these tragedies.


\newpage

# References

Adrian Baddeley, Ege Rubak, Rolf Turner (2015). Spatial Point
Patterns: Methodology and Applications with R. London: Chapman and
Hall/CRC Press, 2015. URL
https://www.routledge.com/Spatial-Point-Patterns-Methodology-and-Applications-with-R/Baddeley-Rubak-Turner/9781482210200/
  
Bivand, Roger S. and Wong, David W. S. (2018) Comparing
implementations of global and local indicators of spatial association
TEST, 27(3), 716-748. URL https://doi.org/10.1007/s11749-018-0599-x

Hellebuyck, M., Halpern, M., Nguyen, T. and Fritze, D., 2018. The State of Mental Health in America. p.9.

Paez A (2021). An Introduction to Spatial Data Analysis and Statistics: A Course in R. McMaster Invisible Press. ISBN: 978-1-7778515-0-7

Pebesma, E., 2018. Simple Features for R: Standardized Support for
Spatial Vector Data. The R Journal 10 (1), 439-446,
https://doi.org/10.32614/RJ-2018-009

Riedman, D., Jernegan, E. and O'Neill, D., 2020. K-12 School Shooting Database. [online] Center for Homeland Defense and Security. Available at: <https://www.chds.us/ssdb/> [Accessed 15 March 2022].

Siegel, M., 2022. State-by-State Firearm Law Data | State Firearm Laws. [online] Statefirearmlaws.org. Available at: <http://www.statefirearmlaws.org/> [Accessed 15 March 2022].


\newpage

# Appendix

```{r ts-plot-1990-2019}
# number of events per month
df2_9019 <- df_9019 %>% 
  mutate(month_year = floor_date(Date, "year"))

ts_9019 <- df2_9019  %>% 
  group_by(month_year) %>%
  summarize(freq = n())

# count of shootings in Apr 1999
freq_1999 <- ts_9019$freq[ts_9019$month_year == as.Date("1999-01-01")]
freq_2006 <- ts_9019$freq[ts_9019$month_year == as.Date("2006-01-01")]
freq_2012 <- ts_9019$freq[ts_9019$month_year == as.Date("2012-01-01")]
freq_2018 <- ts_9019$freq[ts_9019$month_year == as.Date("2018-01-01")]

# plot of shootings
ts_9019 %>% 
  ggplot(aes(x = month_year, y = freq)) +
  geom_line(color = "red") +
  xlab("Date") +
  ylab("Frequency") + 
  scale_x_date(date_labels = "%Y", date_breaks = "2 years") + 
  scale_y_continuous(breaks = seq(0,60, by= 5)) +
  ggtitle("# of K-12 School Shootings per Year, Jan 1990-Dec 2019") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  # adding points to represent certain major events on the timeline
  # this is for Columbine, Apr 1999
  geom_point(aes(x = as.Date("1999-01-01"), y = freq_1999),
             color = "black", size = 2) +
  geom_label(aes(x = as.Date("1999-01-01"), y = freq_1999, 
                label = "Columbine (Apr '99)"),
             color = "black",
             nudge_y = -5, nudge_x = -360, size = 3) +
  # Sandy Hook (Dec 2012)
  geom_point(aes(x = as.Date("2012-01-01"), y = freq_2012), 
             color = "black", size = 2) +
  geom_label(aes(x = as.Date("2012-01-01"), y = freq_2012,
                 label = "Sandy Hook (Dec '12)"), 
             color = "black",
             nudge_y = -2, nudge_x = 1200, size = 3) +
  geom_point(aes(x = as.Date("2006-01-01"), y = freq_2006),
             color = "black", size = 2) +
  geom_label(aes(x = as.Date("2006-01-01"), y = freq_2006,
                 label = "West Nickel Mines (Oct '06)"),
             nudge_y = 2.5, color = "black", size = 3) +
  # Stoneman Douglas (Feb 2018) / Santa Fe (May 2018)
  geom_point(aes(x = as.Date("2018-01-01"), y = freq_2018),
             color = "black", size = 2) +
  geom_label(aes(x = as.Date("2018-01-01"), y = freq_2018,
                 label = "Stoneman Douglas and Santa Fe (Feb/May '18)"),
             color = "black",
             nudge_x = -2800, size = 3)

```

```{r plot-situation, eval = T}
total <- length(df_9019$Incident_ID %>% unique())

situation_count <- df_9019$Situation %>%
  table() %>% 
  as.data.frame() %>%
  rename(Situation = ".") %>%
  arrange(Freq) %>%
  mutate(Pct = Freq / total * 100) %>% 
  mutate(Pct = round(Pct, 1))

# Barplot of number of shootings by situation
ggplot(situation_count, aes(x = Situation, y = Freq),
       alpha = 0.7) + 
  geom_bar(stat = "identity", fill = "darkred") +
  coord_flip() +
  theme_bw() +
  theme(
      panel.border =  element_blank()) +
  geom_text(aes(label = paste0(Freq, ", ", Pct, "%")), 
            hjust= -0.1, color = "black", size = 3.5) +
  scale_y_continuous(limits = c(0, 350)) +
  ggtitle("K-12 School Shootings, by Situation, 1990-2019")
```

```{r event-counts-9019}
# counting the number of events per state, all years
state_count_9019 <- df_9019$State %>%
  table() %>% 
  as.data.frame() %>%
  rename(State = ".") %>% 
  mutate(NAME = covidcast::abbr_to_name(State)) %>%
  add_row(State = "ND", Freq = as.numeric(0), NAME = "North Dakota")


# breaking up the events for 1990-2019 into quantiles
events_breaks_9019 <- classIntervals(
                    c(min(state_count_9019$Freq - 0.5), 
                                state_count_9019$Freq),
                              n = 5, style = "quantile")
events_breaks_9019$brks <- events_breaks_9019$brks %>% round()

# adding the categories into the 1990-2019 state level data
us_states_9019 <- us_states %>% 
  left_join(state_count_9019, by = "NAME") %>%
  replace_na(list(Freq = as.numeric(0))) %>%
  mutate(Freq_cat = cut(Freq, breaks = events_breaks_9019$brks))

us_states_9019$Freq_cat[us_states_9019$State == "ND"] <- as.factor("(0,3]")

us_states_9019 <- us_states_9019 %>% st_as_sf()
```

```{r event-counts-2019}
# filtering only for events in 2019
df_2019 <- df %>% filter(Date >= "2019-01-01" & Date <= "2019-12-31")

# count of events per state, 2019
state_count_2019 <- df_2019$State %>%
  table() %>% 
  as.data.frame() %>%
  rename(State = ".") %>% 
  mutate(NAME = covidcast::abbr_to_name(State)) %>% 
  add_row(State = "ND", Freq = 0, NAME = "North Dakota")

# adding the categories into the 2019 state level data
us_states_2019 <- us_states %>% 
  left_join(state_count_2019, by = "NAME") %>% 
  replace_na(list(Freq = 0)) %>%
  mutate(Freq_cat = factor(Freq, labels = c(0,1,2,3,5), ordered = TRUE)) %>%
  st_as_sf()
```


```{r gun-control-2019}
# getting the amount of gun control laws per state
gun_control_2019 <- gun_control %>%
  rename(NAME = state) %>%
  filter(year == 2019) %>%
  dplyr::select(NAME, year, lawtotal)

# quantiles on gun control
# library(classInt) is loaded
laws_breaks <- classIntervals(c(min(gun_control_2019$lawtotal - 0.5), 
                                gun_control_2019$lawtotal),
                              n = 5, style = "quantile")
us_states3 <- us_states %>% 
  left_join(gun_control_2019, by = "NAME")%>%
  mutate(law_cat = cut(lawtotal, breaks = laws_breaks$brks)) %>%
  st_as_sf()

```

```{r mental-healthcare-2019, warning = F}
mha_2019 <- read.csv("mha_rank_2019.csv", sep = " ") %>% 
  rename(NAME = State.) %>%
  filter(!(NAME %in% c("Alaska", "Hawaii")))

mhc_breaks <- classIntervals(c(min(mha_2019$Rank - 1), 
                               mha_2019$Rank),
                             n = 5, style = "fixed",
                             fixedBreaks = c(1, 10, 20,
                                             30, 40, 51))

us_states4 <- us_states %>% 
  left_join(mha_2019, by = "NAME") %>%
  mutate(mhc_cut = cut(Rank, breaks = mhc_breaks$brks)) %>%
  st_as_sf()

```

```{r areals-plot-creation}
# RColorBrewer is loaded
Reds <- brewer.pal(5, "Reds")
Reds_rev <- Reds[length(Reds):1]

# plotting the map of events per state all years
plot_state_events_9019 <- ggplot() + 
  geom_sf(data = us_states_9019, aes(fill = factor(Freq_cat))) +
  theme_bw() +
  theme(axis.text.x = element_blank(), 
      axis.ticks.x = element_blank(), 
      axis.text.y = element_blank(),  
      axis.ticks.y = element_blank(),
      panel.border = element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()) + 
  ggtitle("K-12 School Shootings, 1990-2019") +
  scale_fill_manual("Shootings", values = Reds, guide = "legend")

# plotting the map of events per state in 2019
plot_state_events_2019 <- ggplot() + 
  geom_sf(data = us_states_2019, aes(fill = factor(Freq_cat))) +
  theme_bw() +
  theme(axis.text.x=element_blank(), 
      axis.ticks.x=element_blank(), 
      axis.text.y=element_blank(),  
      axis.ticks.y=element_blank(),
      panel.border =  element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()) + 
  ggtitle("K-12 School Shootings, 2019") +
  scale_fill_manual("Shootings", values = Reds, guide = "legend")

# plotting the number of gun control laws per state (areal data)
plot_state_laws_2019 <- ggplot() +
  geom_sf(data = us_states3, aes(fill = factor(law_cat))) +
  theme_bw() +
  theme(axis.text.x=element_blank(), 
      axis.ticks.x=element_blank(), 
      axis.text.y=element_blank(),  
      axis.ticks.y=element_blank(),
      panel.border =  element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()) + 
  ggtitle("Gun Control Laws, 2019") +
  scale_fill_manual("Laws", values = Reds, guide = "legend", 
                    na.translate = F)

plot_mhc_2019 <- ggplot() +
  geom_sf(data = us_states4, aes(fill = factor(mhc_cut))) + 
  theme_bw() + 
  theme(axis.text.x = element_blank(), 
      axis.ticks.x = element_blank(), 
      axis.text.y = element_blank(),  
      axis.ticks.y = element_blank(),
      panel.border = element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()) + 
  ggtitle("Mental Healthcare Ranking, 2019") +
  scale_fill_manual("Rank", values = Reds, guide = "legend", 
                    na.translate = F)
```

```{r plot-areals-2019}
plot_state_events_9019
plot_state_events_2019
plot_state_laws_2019
plot_mhc_2019
```

```{r plot-fit1-inhomogeneous}
# fitting inhomogenous ppp that varies only according to coordinates
# ppp does not like marked objects so it's being unmarked here
par(mfrow = c(1,1))
ppp_9019_um <- unmark(ppp_9019)
fit1 <- ppm(ppp_9019_um ~ x + y)

# fitted plot
plot(fit1, se = F, how = "image",
     main = "fitted trend of the simple (x,y) model")
```

```{r fit1-env}
# fit1_env <- envelope(ppp_9019_um, fun = Kinhom,
#                     funarges = list(lambda=fit1),
#                     global = T, nsim = 10, verbose = F)
# plot(fit1_env, xlim = c(0,3))
```

```{r heatmap-preplanned}
# heatmap of preplanned events
dens_9019_preplanned <- density.ppp(ppp_9019[ppp_9019$marks == "Yes"])
plot(dens_9019_preplanned, 
     main = "Heatmap of Preplanned School Shootings, 1990-2019")
contour(dens_9019_preplanned, add = T)
```
