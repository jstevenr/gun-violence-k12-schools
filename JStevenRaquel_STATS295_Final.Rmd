---
title: "Modeling K-12 school shootings as a clustered point process and an areal process, from 1990-2019"

blinded: 0

authors: 
- name: J Steven Raquel
  affiliation: Department of Statistics, University of California, Irvine

abstract: |
The prevalence of gun violence in schools in the United States has been referred to both as an "epidemic" and a public health crisis, and one that has steadily increased over the past several decades (see Figure \@ref(fig:ts-plot-1990-2019)). The debate on how to curb these tragedies is a solidly partisan issue, with calls for more and stronger gun control laws, as well as more mental health care, sometimes as an alternative to gun control. Concealed carry laws are argued both in favor of, and against and are labeled as both an enabler and a potential deterrent to mass shootings. Apart from the trauma that such an event can bring to a community, there is also resonant fear that such incidents inspire "copycat" events on a local and a larger scale. This spatial analysis attempts to model the incidence of these shootings as a Poisson point process, in order to ascertain whether the locations and events occur with complete spatial randomness. We also model the data in an areal form on a per-state basis so as to compare it against the level of gun control and mental health care, also on a state level.

output: 
  bookdown::pdf_book: 
    base_format: rticles::asa_article
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F)
library(tidyverse)
library(lubridate)
# spatial data libraries
library(sf)
library(sp)
library(spdep)
library(raster) 
library(maps) # to get map shapefiles
library(maptools)
library(spatstat) # spatial statistics
library(plotrix)
library(ggmap) # for utilizing Google Maps API 
library(splancs)
library(patchwork) # for plotting ggplots
library(viridis)
library(RColorBrewer)
library(classInt)
library(bookdown)
```

```{r read_incidents_old, eval = F, include = F}
incidents <- read_csv("ss_incidents.csv", na = c("", "null", "N/A"),
                      show_col_types = F)

# incident dataset filtered
incidents_f <- incidents %>%
  dplyr::select(Incident_ID, Date, Quarter, School, Narrative,
                City, State, School_Level, Location, Location_Type, 
                During_School, Time_Period, Situation, 
                Bullied, Domestic_Violence, Gang_Related, 
                Preplanned, Shots_Fired) %>%
  mutate(Date = as.Date(Date)) %>%
  filter(!(Situation %in% c("Accidental", "Suicide/Attempted",
                            "Intentional Property Damage"))
         ) %>%
  filter(Gang_Related == "No") %>% 
  unite(Full_Location, c(School, City, State), remove = F, sep = " ") %>%
  mutate(Full_Location = as.character(Full_Location)) %>%
  filter(!(State %in% c("AK", "HI"))) %>%
  replace_na(list(Situation = "Unknown"))

incidents_f <- incidents_f %>% 
  mutate_at(setdiff(names(incidents_f), c("Incident_ID", "Full_Location", "Date")), 
               .funs = as.factor)
```

```{r get-lat-lon, eval = F, include = F}
# using Google Maps API to search up the lat/lon of all the schools
#incidents_f2 <- incidents_f %>% 
#  mutate_geocode(Full_Location)
# write.csv(incidents_f2, "ss_incidents2.csv", row.names = F)
```

```{r get-perps-weapons, eval = F}
perps <- read_csv("ss_perps.csv", na = c("", "null", "N/A"), 
                  show_col_types = F)
weapons <- read_csv("ss_weapons.csv", na = c("", "null", "N/A"),
                    show_col_types = F)

gun_control <- read_csv("gun_control_database.csv", col_types = "cf")

# perpetrator dataset filtered
perps_f <- perps %>% 
  dplyr::select(incidentid, age, gender, race, schoolaffiliation) %>%
  rename(Incident_ID = incidentid, Age = age, Gender = gender, 
         Race = race, School_Affiliation = schoolaffiliation)

# weapon dataset filtered
weapons_f <- weapons %>%
  dplyr::select(incidentid, weapontype) %>%
  rename(Incident_ID = incidentid, Weapon_Type = weapontype)

# adding perpetrator information to the dataset
incidents_f3 <- left_join(incidents_f2, perps_f, by = "Incident_ID")
# adding weapon information to the dataset
incidents_f4 <- left_join(incidents_f3, weapons_f, by = "Incident_ID")

```

```{r reading_data, warnings = F}
df <- read_csv("ss_incidents2.csv",
                  col_types = "ciDffffffffffffffffdd")

df_9019 <- df %>% filter(Date >= "1990-01-1" & Date <= "2019-12-31")
df_90S <- df %>% filter(Date >= "1990-01-01" & Date <= "1999-12-31")
df_00S <- df %>% filter(Date >= "2000-01-01" & Date <= "2009-12-31")
df_10S <- df %>% filter(Date >= "2010-01-01" & Date <= "2019-12-31")
```

```{r sf-wrangling}
# sf is loaded
# CRS for albers is 5070
# default for us states is 4269
data(us_states)

sf_9019 <- st_as_sf(df_9019, coords = c("lon", "lat")) %>% 
  st_set_crs(4269)

sf_90S <- st_as_sf(df_90S, coords = c("lon", "lat")) %>%
  st_set_crs(4269)

sf_00S <- st_as_sf(df_00S, coords = c("lon", "lat")) %>%
  st_set_crs(4269)

sf_10S <- st_as_sf(df_10S, coords = c("lon", "lat")) %>% 
  st_set_crs(4269)

```

```{r cities-cleaning}
cities_df <- df %>% dplyr::select(City, State, lon, lat) %>% distinct()

# names of cities with more than one shooting from 1990-2022
cities_multi <- table(cities_df$City)%>% 
  as.data.frame() %>%
  rename(City = Var1) %>%
  filter(Freq > 1)
cities_multi_names <- cities_multi$City

# the distinct names and coordinates of cities with >1 shooting
cities_df2 <- cities_df %>% 
  unite(City_State, c(City, State), sep = ", ", remove = F) %>%
  filter(City %in% cities_multi_names) %>% 
  arrange(City_State) %>%
  distinct(City, .keep_all = T)

cities_sf <- st_as_sf(cities_df2, coords = c("lon", "lat")) %>%
  st_set_crs(4269) %>%
  mutate(x = purrr::map_dbl(geometry,1),
         y = purrr::map_dbl(geometry,2))
```

# Data

The school shooting data was sourced directly from the "K-12 School Shooting Database" made available by the Center for Homeland Defense and Security (CHDS). The information that comprises the dataset was determined by a specific process which entailed asking what exactly comprises a school shooting. Most shootings that were premeditated and resulted in mass casualties, such as those that occurred at Columbine in 1999, Sandy Hook in 2012, and Stoneman Douglas in 2018, are considered "indiscriminate shootings". Although the original database contains shootings ruled accidental (from misuse of a firearm) as well as incidences of gang-related gun violence on school grounds, among other incidents, we did not opt to consider this data as relevant to this study in particular. 

```{r ts-plot-1990-2019, fig.cap = "Time series plot of number of K-12 school shootings per year, from Jan 1990 to Dec 2019."}
# number of events per month
df2_9019 <- df_9019 %>% 
  mutate(month_year = floor_date(Date, "year"))

ts_9019 <- df2_9019  %>% 
  group_by(month_year) %>%
  summarize(freq = n())

# count of shootings in Apr 1999
freq_1999 <- ts_9019$freq[ts_9019$month_year == as.Date("1999-01-01")]
freq_2006 <- ts_9019$freq[ts_9019$month_year == as.Date("2006-01-01")]
freq_2012 <- ts_9019$freq[ts_9019$month_year == as.Date("2012-01-01")]
freq_2018 <- ts_9019$freq[ts_9019$month_year == as.Date("2018-01-01")]

# plot of shootings
ts_9019 %>% 
  ggplot(aes(x = month_year, y = freq)) +
  geom_line(color = "red") +
  xlab("Date") +
  ylab("Frequency") + 
  scale_x_date(date_labels = "%Y", date_breaks = "2 years") + 
  scale_y_continuous(breaks = seq(0,60, by= 5)) +
  ggtitle("# of K-12 School Shootings per Year, Jan 1990-Dec 2019") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  # adding points to represent certain major events on the timeline
  # this is for Columbine, Apr 1999
  geom_point(aes(x = as.Date("1999-01-01"), y = freq_1999),
             color = "black", size = 2) +
  geom_label(aes(x = as.Date("1999-01-01"), y = freq_1999, 
                label = "Columbine (Apr '99)"),
             color = "black",
             nudge_y = -5, nudge_x = -360, size = 3) +
  # Sandy Hook (Dec 2012)
  geom_point(aes(x = as.Date("2012-01-01"), y = freq_2012), 
             color = "black", size = 2) +
  geom_label(aes(x = as.Date("2012-01-01"), y = freq_2012,
                 label = "Sandy Hook (Dec '12)"), 
             color = "black",
             nudge_y = -2, nudge_x = 1200, size = 3) +
  geom_point(aes(x = as.Date("2006-01-01"), y = freq_2006),
             color = "black", size = 2) +
  geom_label(aes(x = as.Date("2006-01-01"), y = freq_2006,
                 label = "West Nickel Mines (Oct '06)"),
             nudge_y = 2.5, color = "black", size = 3) +
  # Stoneman Douglas (Feb 2018) / Santa Fe (May 2018)
  geom_point(aes(x = as.Date("2018-01-01"), y = freq_2018),
             color = "black", size = 2) +
  geom_label(aes(x = as.Date("2018-01-01"), y = freq_2018,
                 label = "Stoneman Douglas and Santa Fe (Feb/May '18)"),
             color = "black",
             nudge_x = -2800, size = 3)

```

It was a deliberate decision on our part to filter out these incidents as others like it so as to hone in one the more specific and cultural recognized definition of a school shooting, which is an attack that takes place on school grounds, in order to target students, faculty, or passersby with the intent to cause terror and/or inflict harm on specific individuals. Targeted events related to domestic situations (e.g. anger over a break-up), or the escalation of disputes (e.g. fistfights in which one person pulls out a firearm) were also ruled school shootings.  

The database originally goes as far back as 1970 all the way to present day (March 2022 at the time of this writing), but it was of particular interest to look at the data after 1990 since this consolidates the modern era in which access to firearms and the incidence of gun violence in schools is more normalized. It was also decided to leave out data past 2019 as the COVID-19 pandemic which hit the US in 2020 is a huge confounding variable as it caused many schools to institute remote learning which decreased the incidence of shootings on school campuses.

The data originally only contained the name of the school, as well as the city and state where the events took place, but we were able to use the Google Maps API to source the specific latitude/longitude coordinates of these events.

In addition to this school shooting data, we also have gun control data consisting of several columns of binary categorical data as to whether or not a certain law (e.g. a ban on concealed carry, background checks) is instituted in a given state for the years between 1991 and 2020 from StateFirearmLaws.org, as well as mental healthcare data from 2019 courtesy of Mental Health America (MHA), which consists of a ranking of all 50 states and the District of Columbia on their ratio of need for and accessibility to mental healthcare. More information on these resources can be found on their respective websites (see References section). 

# Methods

## Exploratory Data Analysis

As a preliminary analysis, we chose to plot this data on both a point and at an areal level, for a number of reasons. For one, modeling the data as a point pattern allows us to test the null hypothesis for complete spatial randomness against the alternative hypothesis of clustering. It allows us to hone in on exactly where within a given state might be considered a "hotspot" for a shooting. Conversely, the areal data allows us to compare the number of incidents in a given state and look at it with respect to the level of gun control and mental healthcare available there as well.


```{r plot-point-pattern-creation, echo = F}
# looking at 3 decades of school shootings
# 1990-1999
ss_plot_90s <- ggplot() + 
  geom_sf(data = us_states) +
  geom_sf(data = sf_90S, size = 0.75, col = "red") + 
#  geom_sf(data = cities_sf, color = "black", size = 1) +
#  geom_text_repel(data = cities_sf, 
#                  aes(x = x, y = y, label = City_State),
#                  hjust = 1, vjust = -0.25, size = 2.5) +
  theme(axis.title = element_blank()) +
  ggtitle("1990-99, Total: 147") +
  theme_bw() +
  theme(
    axis.title = element_blank(),
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank(), 
    axis.text.y = element_blank(),  
    axis.ticks.y = element_blank(),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank())

# 2000-2009
ss_plot_00s <- ggplot() + 
  geom_sf(data = us_states) +
  geom_sf(data = sf_00S, size = 0.75, col = "red") + 
#  geom_sf(data = cities_sf, color = "black", size = 1) +
#  geom_text_repel(data = cities_sf, 
#                  aes(x = x, y = y, label = City_State),
#                  hjust = 1, vjust = -0.25, size = 2.5) +
  theme(axis.title = element_blank()) +
  ggtitle("2000-09, Total: 186") +
  theme_bw() +
  theme(
    axis.title = element_blank(),
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank(), 
    axis.text.y = element_blank(),  
    axis.ticks.y = element_blank(),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank())

# 2010-2019
ss_plot_10s <- ggplot() + 
  geom_sf(data = us_states) +
  geom_sf(data = sf_10S, size = 0.75, col = "red") + 
#  geom_sf(data = cities_sf, color = "black", size = 1) +
#  geom_text_repel(data = cities_sf, 
#                  aes(x = x, y = y, label = City_State),
#                  hjust = 1, vjust = -0.25, size = 2.5) +
  ggtitle("2010-19, Total: 240") + 
  theme_bw() +
  theme(
    axis.title = element_blank(),
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank(), 
    axis.text.y = element_blank(),  
    axis.ticks.y = element_blank(),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank())

# 1990-2019
ss_plot_9019 <- ggplot() + 
  geom_sf(data = us_states) +
  geom_sf(data = sf_9019, size = 0.75, col = "red") + 
#  geom_sf(data = cities_sf, color = "black", size = 1) +
#  geom_text_repel(data = cities_sf, 
#                  aes(x = x, y = y, label = City_State),
#                  hjust = 1, vjust = -0.25, size = 2.5) +
  ggtitle("1990-2019, Total: 573") + 
  theme_bw() +
  theme(
      axis.title = element_blank(),
      axis.text.x=element_blank(), 
      axis.ticks.x=element_blank(), 
      axis.text.y=element_blank(),  
      axis.ticks.y=element_blank(),
      panel.border =  element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank())
```

```{r plots-all-four, fig.cap = "Point pattern data for the three separate decades as well as all 30 years."}
# patchwork is loaded
ss_plot_90s + ss_plot_00s + ss_plot_10s + ss_plot_9019 +
  plot_annotation(
    title = "Shootings in K-12 schools in the US"
  )
```

As we can see from \@ref(fig:plots-all-four), the total number of shootings per decade has not only steadily increased over the past 30 years, but the events also occur in and around the same places, which gives credence to our hypothesis that the events exhibit a clustered point pattern. We notice in fact, that there are areas that seem relatively untouched by school shootings in the western United States, whereas shootings all across the South, Midwest and the East Coast recur at a frightening rate. While the West Coast is somewhat blighted by school shootings, particularly in the San Francisco and Los Angeles metropolitan areas, along with major cities in the Pacific Northwest), it is not nearly at the rate experience by the other side of the US. 

```{r plot-situation}
total <- length(df_9019$Incident_ID %>% unique())

situation_count <- df_9019$Situation %>%
  table() %>% 
  as.data.frame() %>%
  rename(Situation = ".") %>%
  arrange(Freq) %>%
  mutate(Pct = Freq / total * 100) %>% 
  mutate(Pct = round(Pct, 1))


ggplot(situation_count, aes(x = Situation, y = Freq),
       alpha = 0.7) + 
  geom_bar(stat = "identity", fill = "darkred") +
  coord_flip() +
  theme_bw() +
  theme(
      panel.border =  element_blank()) +
  geom_text(aes(label = paste0(Freq, ", ", Pct, "%")), 
            hjust= -0.1, color = "black", size = 3.5) +
  scale_y_continuous(limits = c(0, 350)) +
  ggtitle("K-12 School Shootings, by Situation, 1990-2019")
```

```{r event-counts-2019}
df_2019 <- df %>% filter(Date >= "2019-01-01" & Date <= "2019-12-31")

# counting the number of events per state
state_count <- df_2019$State %>%
  table() %>% 
  as.data.frame() %>%
  rename(State = ".") %>% 
  mutate(NAME = covidcast::abbr_to_name(State))

# quantiles on shootings in 2019
us_states2 <- us_states %>% 
  left_join(state_count, by = "NAME") %>% 
  replace_na(list(Freq = 0)) %>%
  mutate(Freq_cat = factor(Freq, labels = c(0,1,2,3,5), ordered = TRUE)) %>%
  st_as_sf()

```

```{r gun-control-2019}
# getting the amount of gun control laws per state
gun_control_2019 <- gun_control %>%
  rename(NAME = state) %>%
  filter(year == 2019) %>%
  dplyr::select(NAME, year, lawtotal)

# quantiles on gun control
# library(classInt) is loaded
laws_breaks <- classIntervals(c(min(gun_control_2019$lawtotal - 0.5), 
                                gun_control_2019$lawtotal),
                              n = 5, style = "quantile")
us_states3 <- us_states %>% 
  left_join(gun_control_2019, by = "NAME")%>%
  mutate(law_cat = cut(lawtotal, breaks = laws_breaks$brks)) %>%
  st_as_sf()

```

```{r mental-healthcare-2019}
mha_2019 <- read.csv("mha_rank_2019.csv", sep = " ") %>% 
  rename(NAME = State.) %>%
  filter(!(NAME %in% c("Alaska", "Hawaii")))

mhc_breaks <- classIntervals(c(min(mha_2019$Rank - 1), 
                               mha_2019$Rank),
                             n = 5, style = "fixed",
                             fixedBreaks = c(1, 10, 20,
                                             30, 40, 51))

us_states4 <- us_states %>% 
  left_join(mha_2019, by = "NAME") %>%
  mutate(mhc_cut = cut(Rank, breaks = mhc_breaks$brks))
```

```{r plot-areals-2019, fig.cap = "A comparison of the incidence of school shootings against gun control and mental health care, in 2019.Darker shades of red imply more shootings, less gun control laws, and less access to mental healthcare."}
# RColorBrewer is loaded
Reds <- brewer.pal(5, "Reds")
Reds_rev <- Reds[length(Reds):1]

# plotting the map of events per state (areal data)
plot_state_incs <- ggplot() + 
  geom_sf(data = us_states2, aes(fill = factor(Freq_cat))) +
  theme_bw() +
  theme(axis.text.x=element_blank(), 
      axis.ticks.x=element_blank(), 
      axis.text.y=element_blank(),  
      axis.ticks.y=element_blank(),
      panel.border =  element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()) + 
  ggtitle("Number of School Shootings per State, 2019") +
  scale_fill_manual("Shootings", values = Reds, guide = "legend")

# plotting the number of gun control laws per state (areal data)
plot_state_laws <- ggplot() +
  geom_sf(data = us_states3, aes(fill = factor(law_cat))) +
  theme_bw() +
  theme(axis.text.x=element_blank(), 
      axis.ticks.x=element_blank(), 
      axis.text.y=element_blank(),  
      axis.ticks.y=element_blank(),
      panel.border =  element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()) + 
  ggtitle("Number of Gun Control Laws per State, 2019") +
  scale_fill_manual("Laws", values = Reds_rev, guide = "legend", 
                    na.translate = F)

plot_mhc <- ggplot() +
  geom_sf(data = us_states4, aes(fill = factor(mhc_cut))) + 
  theme_bw() + 
  theme(axis.text.x = element_blank(), 
      axis.ticks.x = element_blank(), 
      axis.text.y = element_blank(),  
      axis.ticks.y = element_blank(),
      panel.border = element_blank(), 
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()) + 
  ggtitle("Ranking of Mental Healthcare by State, 2019") +
  scale_fill_manual("Rank", values = Reds, guide = "legend", 
                    na.translate = F)
  

# return plot
plot_state_incs + 
  plot_state_laws + 
  plot_mhc + 
  plot_layout(ncol= 1)
# display.brewer.all(n = 5)
```

## Areal Data Visualization

Looking at \@ref (fig:plot-areals-2019), we see that despite the conspicuous lack of gun control or adequate mental healthcare in the northwestern United States, e.g. Idaho, Wyoming, Montana, there were few to no school shootings events in 2019, which is consistent with what we saw in the point pattern visualization as well. This may lend credence to a hypothesis that in fact, a lack of gun control laws and/or mental healthcare may not coincide with a greater incidence of shootings at all. 2019 was selected as the year to analyze in this fashion because it was the year with the highest number of shootings between 1990 and 2019.

Conversely, we do note that many states in the southern region of the US, e.g. Texas, Mississippi, Alabama, do experience greater levels of gun violence in tandem with their lack of gun control or mental healthcare relative to other parts of the country. 

A subset of the northeastern US shows an interesting pattern in that there were states with absolutely no school shootings (Maine, Vermont, New Hampshire) in 2019 despite a lack of strong gun control, but this was in fact in tandem with a relatively high ranking of access to mental healthcare. 

The remainder of this literature will focus on the clustering Poisson point process model for the data, but analysis on an areal level will be left open-ended as potential future work to explore. 

## Point Pattern Analysis

With respect to the coordinate-level data, as with any spatial point pattern analysis, we are concerned with the following three questions, 1) whether the points are located at random, 2) whether they are clustered, and 3) whether they are placed regularly. The hypothesis of _complete spatial randomness_ (CSR) asserts the following:

* The number of events in any region $S$ with area $|S|$ follows a Poisson distribution with mean $\lambda |S|$, where $\lambda$ is the intensity, i.e. $\lambda$ does not change over $S$
* Given $n$ events in $S$, the points $s_i$ are independently located according to the uniform distribution on $S$, i.e. there is no interaction amongst events.

The intensity function $\lambda(s)$, also known as the first-order property of the spatial point process, is defined as

$$\lambda(s) = \lim_{|\Delta s| \to 0} \frac{E[N(\Delta s)]}{| \Delta s|}$$

Firstly we want to ascertain whether the incidences of school shootings are indeed a Poisson process, and if so, determine whether or not the process is homogeneous (where the intensity function $\lambda(s)$ assumes a constant $\lambda$) or inhomogeneous. The intensity function is the expected number of events per unit area, so a constant $\lambda$ would mean that the events occur at a consistent rate across the whole spatial area. This could be in a patterned fashion, or completely random.

## Quadrats

While we could estimate the intensity function across the entire area, what we are interested in this particular analysis is to how the intensity varies across different regions contained therein. We do this by splitting up the area into what are referred to as _quadrats_.

```{r states-ppp, warning = F}
require(readr)
require(maps)
require(spatstat)
require(spatstat.geom)

# reading the data
# from readr (part of tidyverse)
df <- read_csv("ss_incidents2.csv",
                  col_types = "ciDffffffffffffffffdd")

df_9019 <- df %>% filter(Date >= "1990-01-1" & Date <= "2019-12-31")

## getting shapefile of the US
us_map <- map("usa",plot=F)

## These are the vectors with the longitudes and latitudes
us_boundaries_x <- us_map$x
us_boundaries_y <- us_map$y

## Creating the matrix with the longitude and latitude of the boundaries
us_poly <- matrix(cbind(us_boundaries_x[1:6886],us_boundaries_y[1:6886]),
                        nrow=6886,ncol=2)
## Creating the window of the spatial point pattern.
us_win <- owin(poly=us_poly)

# index of points inside the window
is_in <- inside.owin(x = df_9019$lon,
                     y = df_9019$lat,
                     us_win)

# subsetting the points only inside the window
df_9019_2 <- df_9019[is_in,]

# creating ppp objects with the coordinates and the windows
ppp_9019 <- ppp(x = df_9019$lon, y = df_9019$lat,
                window = us_win)
```

```{r plot-quadrats-1990-2019, fig.cap = "Quadrat plot of the school shooting incidents in whole continental US, 1990-2019"}
quadrat_9019 <- quadratcount(ppp_9019)
plot(ppp_9019, axes = F, 
     main = "Quadrat Plot of Incidents, 1990-2019", 
     cols= rgb(0,0,0,.2), pch = 20)
plot(quadrat_9019, add = TRUE)
```

Figure \@ref(fig:plot-quadrats-1990-2019) demonstrates that a great deal of events occur predominantly particularly in the southern and eastern United States. It is important to note however that within a given quadrat, the number of events contained therein does not give us information about how clustered the events are e.g. the 32 events in the quadrat containing mostly Florida do seem to be distributed rather randomly, whereas the 29 events in the quadrat comprising the north of California and Nevada appear rather clustered. Naturally, the count of events contained within each quadrat is also dependent on the definition of these dimensions, so they are very subject to misleading conclusions based on these dimensions. Naturally since the continental United States is not shaped like a simple polygon, dividing it into a reasonable number of equitable quadrats is not an easy task.

One methodology for testing for clustering is a Monte Carlo quadrat test in which we take a number of simulations of patterns similar to our own, and compare it to the distribution of the pattern exhibited under the null hypothesis i.e. a homogeneous Poisson point pattern. The alternative hypothesis and the test-statistic varies depending on the test. 

```{r quadrat-test}
# Ha: the pattern is not completely random
q_test_CSR <- quadrat.test(ppp_9019, alternative = "two.sided",
  method = "MonteCarlo", nsim = 4999, conditional = T)

# Ha: the PP is clustered
q_test_clustered <- quadrat.test(ppp_9019, alternative = "clustered",
             method = "MonteCarlo", nsim = 4999, conditional = T)

q_test_CSR
q_test_clustered
```
## Kernel Density

```{r bandwidth-calculate, cache = T}
# optimal bandwidth for the kernel density of each point pattern
bw_opt_9019 <- bw.diggle(ppp_9019)
```


```{r density-90-19}
dens_9019 <- density(ppp_9019, bw = bw_opt_9019)
plot(dens_9019, main = "Density of School Shootings, 1990-2019")
contour(dens_9019, add = T)
```

## Nearest Neighbor

```{r nearest-neighbor}
ppp_nn <- nndist(ppp_9019)

ggplot(data = data.frame(dist = ppp_nn),
       aes(dist)) +
  geom_histogram(binwidth = 0.5)

```

```{r Sienen-diagram}
ppp_9019 %mark% (ppp_nn) %>%
  plot(markscale = 1, main = "Stienen diagram, 1990-2019 shootings")
```


```{r K-function}
# empirical function is the solid black line
# dashed red line is theoretical
K_9019 <- Kest(ppp_9019, correction = "none")
plot(K_9019, main = "K function")
```

```{r L-function}
# L_9019 <- Lest(ppp_9019, correction = "none")
# plot(L_9019, . - r ~ r, main = "L function")
# env_L <- envelope(ppp_9019, fun = Lest, nsim = 20)
# plot_env_L <- plot(env_L, . - r ~ r, main = "envelope of L function")
plot_env_L <- imager::load.image("plot_envelope_L.png")
plot_env_L
```

```{r G-function}
# empirical function is the solid black line
# dashed red line is theoretical
G_9019 <- Gest(ppp_9019, correction = "none")
plot(G_9019, main = "G function")
```

```{R KL-functions, cache = T}
# 1990s
# K_9019 <- Kest(ppp_9019)
L_9019 <- Lest(ppp_9019)
plot(L_9019, . - r ~ r, main = "Plot of L function")
```

```{R envelopes, cache = T, results = 'hide', eval = F}
# envelopes
#env_K <- envelope(ppp_9019, fun = Kest, nsim = 99)
env_L <- envelope(ppp_9019, fun = Lest, nsim = 20)
```
```{r plot-envelope, eval = F}
#plot(env_K, main = "K-function envelope")
plot(env_L, . - r ~ r, main = "L function envelope")
```
## Modeling the intensity function using categorical covariates

```{r ppm_model, eval = F}
# inhomogenous Poisson process

# fitting ppm
fit1 <- ppm(ppp_9019 ~ Bullied + Preplanned, 
            data = df_9019_2)
```

```{r log-gaussian-cox, eval = F}
logcp_9019 <- lgcp.estK(ppp_9019, c(sigma2 = 10, alpha = 2))
plot(logcp_9019, 
     main = "Fitted K function and theoretical K function \n log Gaussian-Cox process")
```

```{r cluster-fit}
clusterfit1 <- clusterfit(ppp_9019, clusters = "Cauchy")

```


# Discussion

















\newpage

# References

\newpage

# Appendix

```{r ppp-decades, eval = F}
matrix_90s <- matrix(cbind(df_90S$lon,df_90S$lat), 
                     nrow = length(df_90S$lat), ncol = 2)
matrix_00s <- matrix(cbind(df_00S$lon, df_00S$lat),
                     nrow = length(df_00S$lat), ncol = 2)
matrix_10s <- matrix(cbind(df_10S$lon, df_10S$lat), 
                     nrow = length(df_10S$lat), ncol = 2)
ppp_90s <- as.ppp(matrix_90s, us_win)
ppp_00s <- as.ppp(matrix_00s, us_win)
ppp_10s <- as.ppp(matrix_10s, us_win)
```

```{r out-points, eval = F}
library(splancs)

plot(us_boundaries_x[1:6887],us_boundaries_y[1:6887], 
     type="l",col="black",xlab="Longitude",ylab="Latitude")


# rejects
rejects_9019 <- attr(ppp_9019, "rejects")
rejects_90s <- attr(ppp_90s, "rejects")
rejects_00s <- attr(ppp_00s, "rejects")
rejects_10s <- attr(ppp_10s, "rejects")

# index of points inside the window
is_in <- inside.owin(x = matrix_9019[,1],
                     y = matrix_9019[,2],
                     us_win)

# data.frame of points in/out
point_in <- df_9019[is_in,]
point_out <- df_9019[!is_in,]
```

```{r bw-decades, eval = F}
bw_opt_90s <- bw.diggle(ppp_90s) 
bw_opt_00s <- bw.diggle(ppp_00s) 
bw_opt_10s <- bw.diggle(ppp_10s) 
```
```{r plot-ppp-90s, eval = F}
quadrat_90s <- quadratcount(ppp_90s)
plot(ppp_90s, axes = F, 
     main = "Poisson Point Pattern, School Shootings 1990s", 
     cols= rgb(0,0,0,.2), pch = 20)
plot(quadrat_90s, add = TRUE)
```

```{r plot-ppp-00s, eval = F}
quadrat_00s <- quadratcount(ppp_00s)
plot(ppp_00s, axes = F, 
     main = "Poisson Point Pattern, School Shootings 2000s", 
     cols= rgb(0,0,0,.2), pch = 20)
plot(quadrat_00s, add = TRUE)
```

```{r plot-ppp-2010s, eval = F}
quadrat_10s <- quadratcount(ppp_10s)
plot(ppp_10s, axes = F, 
     main = "Poisson Point Pattern, School Shootings 2010s", 
     cols= rgb(0,0,0,.2), pch = 20)
plot(quadrat_10s, add = TRUE)
```

```{r quadrat-test-decades, eval = F}
quadrat.test(ppp_90s, alternative = "clustered",
             method = "MonteCarlo", nsim = 4999, conditional = T)

quadrat.test(ppp_00s, alternative = "clustered",
             method = "MonteCarlo", nsim = 4999, conditional = T)

quadrat.test(ppp_10s, alternative = "clustered",
             method = "MonteCarlo", nsim = 4999, conditional = T)

```


```{r density-90s, eval = F}
# cross-validated bandwidth selection for finding optimal bandwidth
dens_90s <- density(ppp_90s, bw = bw_opt_90s)
plot(dens_90s, main = "Density of School Shootings, 1990-1999")
contour(dens_90s, add = T)
```
```{r density-00s, eval = F}
dens_00s <- density(ppp_00s, bw = bw_opt_00s)
plot(dens_00s, main = "Density of School Shootings, 2000-2009")
contour(dens_00s, add = T)
```

```{r density-10s, eval = F}
dens_10s <- density(ppp_10s, bw = bw_opt_10s)
plot(dens_10s, main = "Density of School Shootings, 2010-2019")
contour(dens_10s, add = T)
```
